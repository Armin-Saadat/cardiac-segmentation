{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"voxelmorph_pytorch_main.ipynb","provenance":[],"authorship_tag":"ABX9TyPNjXJWx6I3aQRul4THWIcX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1aNpYBtyp-tN","executionInfo":{"status":"ok","timestamp":1620641524238,"user_tz":-270,"elapsed":26228,"user":{"displayName":"elahe badali","photoUrl":"","userId":"17931211121425855261"}},"outputId":"ee9e9b06-83eb-48d4-8e3c-e856cd4710ec"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CQJRiGD5wXz9","executionInfo":{"status":"ok","timestamp":1620641524239,"user_tz":-270,"elapsed":5514,"user":{"displayName":"elahe badali","photoUrl":"","userId":"17931211121425855261"}},"outputId":"fd23434d-eba4-40ce-fa97-52ae9384ea1d"},"source":["%cd './drive/MyDrive/Cardiac Project/Registration_phase'"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Cardiac Project/Registration_phase\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BOnPakL0putT","executionInfo":{"status":"ok","timestamp":1620641530621,"user_tz":-270,"elapsed":11567,"user":{"displayName":"elahe badali","photoUrl":"","userId":"17931211121425855261"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.distributions.normal import Normal\n","import torch.nn.functional as nnf\n","from torchvision import transforms\n","import torch.optim as optim\n","from sklearn.model_selection import train_test_split\n","from torch.utils import data\n","\n","from tqdm import tqdm\n","import cv2\n","import glob\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") "],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"R0iMjcgzqS2j","executionInfo":{"status":"ok","timestamp":1620641530622,"user_tz":-270,"elapsed":11256,"user":{"displayName":"elahe badali","photoUrl":"","userId":"17931211121425855261"}}},"source":["path = './phase_2_voxel_morph/voxelMorph/'"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"VGk0bumQqS4s","executionInfo":{"status":"ok","timestamp":1620641530623,"user_tz":-270,"elapsed":10915,"user":{"displayName":"elahe badali","photoUrl":"","userId":"17931211121425855261"}}},"source":["def read_dataset(path):\n","    cine_dataset = []\n","    lge_dataset = []\n","    for filename in tqdm(glob.iglob(path + \"**/*.png\" , recursive = True)):\n","        class_type = filename.split('/')[3]\n","        image_name = filename.split('/')[4]\n","        image = cv2.imread(filename ,  cv2.IMREAD_GRAYSCALE)\n","        image = transforms.ToTensor()(image)\n","        if class_type == 'cine_images':\n","            cine_dataset.append((image , image_name))\n","        if class_type == 'lge_images':\n","            lge_dataset.append((image , image_name))\n","\n","    cine_dataset.sort(key = lambda x: x[1]) \n","    lge_dataset.sort(key = lambda x: x[1]) \n"," \n","    dataset = []\n","    for i in range(len(cine_dataset)):\n","        cine_image = cine_dataset[i][0]\n","        lge_image = lge_dataset[i][0]\n","        dataset.append((cine_image , lge_image))\n","\n","    return dataset"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WShAJ_grwLpg","executionInfo":{"status":"ok","timestamp":1620641959627,"user_tz":-270,"elapsed":439599,"user":{"displayName":"elahe badali","photoUrl":"","userId":"17931211121425855261"}},"outputId":"2e2476ee-7613-4c79-b1df-ef57c75363be"},"source":["dataset = read_dataset(path)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["606it [07:09,  1.41it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"lcdbDFTD96Vy","executionInfo":{"status":"ok","timestamp":1620641959630,"user_tz":-270,"elapsed":439265,"user":{"displayName":"elahe badali","photoUrl":"","userId":"17931211121425855261"}}},"source":["train_dataset , test_dataset = train_test_split(dataset, test_size=0.33, random_state=42)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ztMQ1Pa996YU","executionInfo":{"status":"ok","timestamp":1620641959633,"user_tz":-270,"elapsed":438935,"user":{"displayName":"elahe badali","photoUrl":"","userId":"17931211121425855261"}},"outputId":"01d93dad-dabd-4641-a5c8-7a4aab1c15a6"},"source":["train_loader = data.DataLoader(train_dataset ,\n","                               batch_size = 20,\n","                               shuffle = True,\n","                               num_workers =  6,\n","                               worker_init_fn =  np.random.seed(42))  \n","test_loader = data.DataLoader(test_dataset ,\n","                               batch_size = 20,\n","                               shuffle = True,\n","                               num_workers =  6,\n","                               worker_init_fn =  np.random.seed(42))                      "],"execution_count":8,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"QGlnIZ1vqsAn","executionInfo":{"status":"ok","timestamp":1620641959634,"user_tz":-270,"elapsed":438593,"user":{"displayName":"elahe badali","photoUrl":"","userId":"17931211121425855261"}}},"source":["def default_unet_features():\n","    nb_features = [\n","        [16, 32, 32, 32],             # encoder\n","        [32, 32, 32, 32, 32, 16, 16]  # decoder\n","    ]\n","    return nb_features"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"oFDl6NRjrnAr","executionInfo":{"status":"ok","timestamp":1620641959636,"user_tz":-270,"elapsed":438260,"user":{"displayName":"elahe badali","photoUrl":"","userId":"17931211121425855261"}}},"source":["class ConvBlock(nn.Module):\n","    \"\"\"\n","    Specific convolutional block followed by leakyrelu for unet.\n","    \"\"\"\n","\n","    def __init__(self, ndims, in_channels, out_channels, stride=1):\n","        super().__init__()\n","\n","        Conv = getattr(nn, 'Conv%dd' % ndims)\n","        self.main = Conv(in_channels, out_channels, 3, stride, 1)\n","        self.activation = nn.LeakyReLU(0.2)\n","\n","    def forward(self, x):\n","        out = self.main(x)\n","        out = self.activation(out)\n","        return out"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"WJfBiyUppYzX","executionInfo":{"status":"ok","timestamp":1620641959637,"user_tz":-270,"elapsed":437913,"user":{"displayName":"elahe badali","photoUrl":"","userId":"17931211121425855261"}}},"source":["class Unet(nn.Module):\n","    \"\"\"\n","    A unet architecture. Layer features can be specified directly as a list of encoder and decoder\n","    features or as a single integer along with a number of unet levels. The default network features\n","    per layer (when no options are specified) are:\n","\n","        encoder: [16, 32, 32, 32]\n","        decoder: [32, 32, 32, 32, 32, 16, 16]\n","    \"\"\"\n","\n","    def __init__(self, inshape, nb_features=None, nb_levels=None, feat_mult=1):\n","        super().__init__()\n","        \"\"\"\n","        Parameters:\n","            inshape: Input shape. e.g. (192, 192, 192)\n","            nb_features: Unet convolutional features. Can be specified via a list of lists with\n","                the form [[encoder feats], [decoder feats]], or as a single integer. If None (default),\n","                the unet features are defined by the default config described in the class documentation.\n","            nb_levels: Number of levels in unet. Only used when nb_features is an integer. Default is None.\n","            feat_mult: Per-level feature multiplier. Only used when nb_features is an integer. Default is 1.\n","        \"\"\"\n","\n","        # ensure correct dimensionality\n","        ndims = len(inshape)\n","        assert ndims in [1, 2, 3], 'ndims should be one of 1, 2, or 3. found: %d' % ndims\n","\n","        # default encoder and decoder layer features if nothing provided\n","        if nb_features is None:\n","            nb_features = default_unet_features()\n","\n","        # build feature list automatically\n","        if isinstance(nb_features, int):\n","            if nb_levels is None:\n","                raise ValueError('must provide unet nb_levels if nb_features is an integer')\n","            feats = np.round(nb_features * feat_mult ** np.arange(nb_levels)).astype(int)\n","            self.enc_nf = feats[:-1]\n","            self.dec_nf = np.flip(feats)\n","        elif nb_levels is not None:\n","            raise ValueError('cannot use nb_levels if nb_features is not an integer')\n","        else:\n","            self.enc_nf, self.dec_nf = nb_features\n","\n","        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n","\n","        # configure encoder (down-sampling path)\n","        prev_nf = 2\n","        self.downarm = nn.ModuleList()\n","        for nf in self.enc_nf:\n","            self.downarm.append(ConvBlock(ndims, prev_nf, nf, stride=2))\n","            prev_nf = nf\n","\n","        # configure decoder (up-sampling path)\n","        enc_history = list(reversed(self.enc_nf))\n","        self.uparm = nn.ModuleList()\n","        for i, nf in enumerate(self.dec_nf[:len(self.enc_nf)]):\n","            channels = prev_nf + enc_history[i] if i > 0 else prev_nf\n","            self.uparm.append(ConvBlock(ndims, channels, nf, stride=1))\n","            prev_nf = nf\n","\n","        # configure extra decoder convolutions (no up-sampling)\n","        prev_nf += 2\n","        self.extras = nn.ModuleList()\n","        for nf in self.dec_nf[len(self.enc_nf):]:\n","            self.extras.append(ConvBlock(ndims, prev_nf, nf, stride=1))\n","            prev_nf = nf\n"," \n","    def forward(self, x):\n","\n","        # get encoder activations\n","        x_enc = [x]\n","        for layer in self.downarm:\n","            x_enc.append(layer(x_enc[-1]))\n","\n","        # conv, upsample, concatenate series\n","        x = x_enc.pop()\n","        for layer in self.uparm:\n","            x = layer(x)\n","            x = self.upsample(x)\n","            x = torch.cat([x, x_enc.pop()], dim=1)\n","\n","        # extra convs at full resolution\n","        for layer in self.extras:\n","            x = layer(x)\n","\n","        return x\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"lDn6FT2hqAkc","executionInfo":{"status":"ok","timestamp":1620641959638,"user_tz":-270,"elapsed":437555,"user":{"displayName":"elahe badali","photoUrl":"","userId":"17931211121425855261"}}},"source":["class SpatialTransformer(nn.Module):\n","    \"\"\"\n","    N-D Spatial Transformer\n","    \"\"\"\n","\n","    def __init__(self, size, mode='bilinear'):\n","        super().__init__()\n","\n","        self.mode = mode\n","\n","        # create sampling grid\n","        vectors = [torch.arange(0, s) for s in size]\n","        grids = torch.meshgrid(vectors)\n","        grid = torch.stack(grids)\n","        grid = torch.unsqueeze(grid, 0)\n","        grid = grid.type(torch.FloatTensor)\n","\n","        # registering the grid as a buffer cleanly moves it to the GPU, but it also\n","        # adds it to the state dict. this is annoying since everything in the state dict\n","        # is included when saving weights to disk, so the model files are way bigger\n","        # than they need to be. so far, there does not appear to be an elegant solution.\n","        # see: https://discuss.pytorch.org/t/how-to-register-buffer-without-polluting-state-dict\n","        self.register_buffer('grid', grid)\n","\n","    def forward(self, src, flow):\n","        # new locations\n","        new_locs = self.grid + flow\n","        shape = flow.shape[2:]\n","\n","        # need to normalize grid values to [-1, 1] for resampler\n","        for i in range(len(shape)):\n","            new_locs[:, i, ...] = 2 * (new_locs[:, i, ...] / (shape[i] - 1) - 0.5)\n","\n","        # move channels dim to last position\n","        # also not sure why, but the channels need to be reversed\n","        if len(shape) == 2:\n","            new_locs = new_locs.permute(0, 2, 3, 1)\n","            new_locs = new_locs[..., [1, 0]]\n","        elif len(shape) == 3:\n","            new_locs = new_locs.permute(0, 2, 3, 4, 1)\n","            new_locs = new_locs[..., [2, 1, 0]]\n","\n","        return nnf.grid_sample(src, new_locs, align_corners=True, mode=self.mode)\n","\n","\n","class VecInt(nn.Module):\n","    \"\"\"\n","    Integrates a vector field via scaling and squaring.\n","    \"\"\"\n","\n","    def __init__(self, inshape, nsteps):\n","        super().__init__()\n","        \n","        assert nsteps >= 0, 'nsteps should be >= 0, found: %d' % nsteps\n","        self.nsteps = nsteps\n","        self.scale = 1.0 / (2 ** self.nsteps)\n","        self.transformer = SpatialTransformer(inshape)\n","\n","    def forward(self, vec):\n","        vec = vec * self.scale\n","        for _ in range(self.nsteps):\n","            vec = vec + self.transformer(vec, vec)\n","        return vec\n","\n","\n","class ResizeTransform(nn.Module):\n","    \"\"\"\n","    Resize a transform, which involves resizing the vector field *and* rescaling it.\n","    \"\"\"\n","\n","    def __init__(self, vel_resize, ndims):\n","        super().__init__()\n","        self.factor = 1.0 / vel_resize\n","        self.mode = 'linear'\n","        if ndims == 2:\n","            self.mode = 'bi' + self.mode\n","        elif ndims == 3:\n","            self.mode = 'tri' + self.mode\n","\n","    def forward(self, x):\n","        if self.factor < 1:\n","            # resize first to save memory\n","            x = nnf.interpolate(x, align_corners=True, scale_factor=self.factor, mode=self.mode)\n","            x = self.factor * x\n","\n","        elif self.factor > 1:\n","            # multiply first to save memory\n","            x = self.factor * x\n","            x = nnf.interpolate(x, align_corners=True, scale_factor=self.factor, mode=self.mode)\n","\n","        # don't do anything if resize is 1\n","        return x"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"83te6iAtuHjY","executionInfo":{"status":"ok","timestamp":1620641959639,"user_tz":-270,"elapsed":437247,"user":{"displayName":"elahe badali","photoUrl":"","userId":"17931211121425855261"}}},"source":["class VxmDense(nn.Module):\n","    \"\"\"\n","    VoxelMorph network for (unsupervised) nonlinear registration between two images.\n","    \"\"\"\n","\n","    def __init__(self,\n","        inshape,\n","        nb_unet_features=None,\n","        nb_unet_levels=None,\n","        unet_feat_mult=1,\n","        int_steps=7,\n","        int_downsize=2,\n","        bidir=False,\n","        use_probs=False):\n","        \"\"\" \n","        Parameters:\n","            inshape: Input shape. e.g. (192, 192, 192)\n","            nb_unet_features: Unet convolutional features. Can be specified via a list of lists with\n","                the form [[encoder feats], [decoder feats]], or as a single integer. If None (default),\n","                the unet features are defined by the default config described in the unet class documentation.\n","            nb_unet_levels: Number of levels in unet. Only used when nb_features is an integer. Default is None.\n","            unet_feat_mult: Per-level feature multiplier. Only used when nb_features is an integer. Default is 1.\n","            int_steps: Number of flow integration steps. The warp is non-diffeomorphic when this value is 0.\n","            int_downsize: Integer specifying the flow downsample factor for vector integration. The flow field\n","                is not downsampled when this value is 1.\n","            bidir: Enable bidirectional cost function. Default is False.\n","            use_probs: Use probabilities in flow field. Default is False.\n","        \"\"\"\n","        super().__init__()\n","\n","        # internal flag indicating whether to return flow or integrated warp during inference\n","        self.training = True\n","\n","        # ensure correct dimensionality\n","        ndims = len(inshape)\n","        assert ndims in [1, 2, 3], 'ndims should be one of 1, 2, or 3. found: %d' % ndims\n","        # configure core unet model\n","        self.unet_model = Unet(\n","            inshape,\n","            nb_features=nb_unet_features,\n","            nb_levels=nb_unet_levels,\n","            feat_mult=unet_feat_mult\n","        )\n","\n","        # configure unet to flow field layer\n","        Conv = getattr(nn, 'Conv%dd' % ndims)\n","        self.flow = Conv(self.unet_model.dec_nf[-1], ndims, kernel_size=3, padding=1)\n","\n","        # init flow layer with small weights and bias\n","        self.flow.weight = nn.Parameter(Normal(0, 1e-5).sample(self.flow.weight.shape))\n","        self.flow.bias = nn.Parameter(torch.zeros(self.flow.bias.shape))\n","\n","        # probabilities are not supported in pytorch\n","        if use_probs:\n","            raise NotImplementedError('Flow variance has not been implemented in pytorch - set use_probs to False')\n","\n","        # configure optional resize layers\n","        resize = int_steps > 0 and int_downsize > 1\n","        self.resize = ResizeTransform(int_downsize, ndims) if resize else None\n","        self.fullsize = ResizeTransform(1 / int_downsize, ndims) if resize else None\n","\n","        # configure bidirectional training\n","        self.bidir = bidir\n","\n","        # configure optional integration layer for diffeomorphic warp\n","        down_shape = [int(dim / int_downsize) for dim in inshape]\n","        self.integrate = VecInt(down_shape, int_steps) if int_steps > 0 else None\n","\n","        # configure transformer\n","        self.transformer = SpatialTransformer(inshape)\n","\n","    def forward(self, source, target, registration=False):\n","        '''\n","        Parameters:\n","            source: Source image tensor.\n","            target: Target image tensor.\n","            registration: Return transformed image and flow. Default is False.\n","        '''\n","\n","        # concatenate inputs and propagate unet\n","        x = torch.cat([source, target], dim=1)\n","        x = self.unet_model(x)\n","\n","        # transform into flow field\n","        flow_field = self.flow(x)\n","\n","        # resize flow for integration\n","        pos_flow = flow_field\n","        if self.resize:\n","            pos_flow = self.resize(pos_flow)\n","\n","        preint_flow = pos_flow\n","\n","        # negate flow for bidirectional model\n","        neg_flow = -pos_flow if self.bidir else None\n","\n","        # integrate to produce diffeomorphic warp\n","        if self.integrate:\n","            pos_flow = self.integrate(pos_flow)\n","            neg_flow = self.integrate(neg_flow) if self.bidir else None\n","\n","            # resize to final resolution\n","            if self.fullsize:\n","                pos_flow = self.fullsize(pos_flow)\n","                neg_flow = self.fullsize(neg_flow) if self.bidir else None\n","\n","        # warp image with flow field\n","        y_source = self.transformer(source, pos_flow)\n","        y_target = self.transformer(target, neg_flow) if self.bidir else None\n","\n","        # return non-integrated flow field if training\n","        if not registration:\n","            return (y_source, y_target, preint_flow) if self.bidir else (y_source, preint_flow)\n","        else:\n","            return y_source, pos_flow"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"mhYIlBdbp5ai","executionInfo":{"status":"ok","timestamp":1620641959640,"user_tz":-270,"elapsed":436903,"user":{"displayName":"elahe badali","photoUrl":"","userId":"17931211121425855261"}}},"source":["import torch\n","import torch.nn.functional as F\n","import numpy as np\n","import math\n","\n","\n","class NCC:\n","    \"\"\"\n","    Local (over window) normalized cross correlation loss.\n","    \"\"\"\n","\n","    def __init__(self, win=None):\n","        self.win = win\n","\n","    def loss1(self, y_true, y_pred):\n","        \n","        I = y_true\n","        J = y_pred\n","\n","        # get dimension of volume\n","        # assumes I, J are sized [batch_size, *vol_shape, nb_feats]\n","        ndims = len(list(I.size())) - 2\n","        assert ndims in [1, 2, 3], \"volumes should be 1 to 3 dimensions. found: %d\" % ndims\n","        # set window size\n","        win = [9] * ndims if self.win is None else self.win\n","\n","        # compute filters\n","        sum_filt = torch.ones([1, 1, *win]).to(\"cuda\")\n","\n","        pad_no = math.floor(win[0]/2)\n","\n","        # get convolution function\n","        conv_fn = getattr(F, 'conv%dd' % ndims)\n","\n","        # compute CC squares\n","        I2 = I * I\n","        J2 = J * J\n","        IJ = I * J\n","        I_sum = conv_fn(I, sum_filt, stride=(1,1), padding=1)\n","        J_sum = conv_fn(J, sum_filt, stride=(1,1), padding=1)\n","        I2_sum = conv_fn(I2, sum_filt, stride=(1,1), padding=1)\n","        J2_sum = conv_fn(J2, sum_filt, stride=(1,1), padding=1)\n","        IJ_sum = conv_fn(IJ, sum_filt, stride=(1,1), padding=1)\n","\n","        win_size = np.prod(win)\n","        u_I = I_sum / win_size\n","        u_J = J_sum / win_size\n","\n","        cross = IJ_sum - u_J * I_sum - u_I * J_sum + u_I * u_J * win_size\n","        I_var = I2_sum - 2 * u_I * I_sum + u_I * u_I * win_size\n","        J_var = J2_sum - 2 * u_J * J_sum + u_J * u_J * win_size\n","\n","        cc = cross * cross / (I_var * J_var + 1e-5)\n","\n","        return -torch.mean(cc)\n","    \n","    def loss(self, y_true, y_pred):\n","        n = 9\n","        I = y_true\n","        J = y_pred\n","        batch_size, channels, xdim, ydim = I.shape\n","        I2 = torch.mul(I, I)\n","        J2 = torch.mul(J, J)\n","        IJ = torch.mul(I, J)\n","        sum_filter = torch.ones((1, channels, n, n))\n","        sum_filter = sum_filter.to(\"cuda\")\n","        I_sum = torch.conv2d(I, sum_filter, padding=1, stride=(1,1))\n","        J_sum = torch.conv2d(J, sum_filter,  padding=1 ,stride=(1,1))\n","        I2_sum = torch.conv2d(I2, sum_filter, padding=1, stride=(1,1))\n","        J2_sum = torch.conv2d(J2, sum_filter, padding=1, stride=(1,1))\n","        IJ_sum = torch.conv2d(IJ, sum_filter, padding=1, stride=(1,1))\n","        win_size = n**2\n","        u_I = I_sum / win_size\n","        u_J = J_sum / win_size\n","        cross = IJ_sum - u_J*I_sum - u_I*J_sum + u_I*u_J*win_size\n","        I_var = I2_sum - 2 * u_I * I_sum + u_I*u_I*win_size\n","        J_var = J2_sum - 2 * u_J * J_sum + u_J*u_J*win_size\n","        cc = cross*cross / (I_var*J_var + np.finfo(float).eps)\n","        return torch.mean(cc)\n","\n","\n","class MSE:\n","    \"\"\"\n","    Mean squared error loss.\n","    \"\"\"\n","\n","    def loss(self, y_true, y_pred):\n","        return torch.mean((y_true - y_pred) ** 2)\n","\n","\n","class Dice:\n","    \"\"\"\n","    N-D dice for segmentation\n","    \"\"\"\n","\n","    def loss(self, y_true, y_pred):\n","        ndims = len(list(y_pred.size())) - 2\n","        vol_axes = list(range(2, ndims+2))\n","        top = 2 * (y_true * y_pred).sum(dim=vol_axes)\n","        bottom = torch.clamp((y_true + y_pred).sum(dim=vol_axes), min=1e-5)\n","        dice = torch.mean(top / bottom)\n","        return -dice\n","\n","\n","class Grad:\n","    \"\"\"\n","    N-D gradient loss.\n","    \"\"\"\n","\n","    def __init__(self, penalty='l1', loss_mult=None):\n","        self.penalty = penalty\n","        self.loss_mult = loss_mult\n","\n","    def loss(self, _, y_pred):\n","        y_pred = y_pred.permute(0,2,3,1)\n","\n","        dy = torch.abs(y_pred[:, 1:, :, :] - y_pred[:, :-1, :, :]) \n","        dx = torch.abs(y_pred[ :, :, 1:, :] - y_pred[:, :, :-1, :]) \n","\n","        if self.penalty == 'l2':\n","            dy = dy * dy\n","            dx = dx * dx\n","\n","        d = torch.mean(dx) + torch.mean(dy) \n","        grad = d / 2.0\n","\n","        if self.loss_mult is not None:\n","            grad *= self.loss_mult\n","        return grad"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"C-JuYjoOwLr4","executionInfo":{"status":"ok","timestamp":1620641959641,"user_tz":-270,"elapsed":436518,"user":{"displayName":"elahe badali","photoUrl":"","userId":"17931211121425855261"}}},"source":["def train(model , train_loader ,epochs):\n","    lamda = 0.001\n","    for epoch in range(epochs):\n","        epoch_loss = 0\n","        epoch_dice = 0\n","        for fixed_batch , moving_batch in train_loader:\n","            optimizer.zero_grad()\n","            fixed_batch = fixed_batch.to(device)\n","            moving_batch = moving_batch.to(device)\n","            registered_images , flow = model(moving_batch , fixed_batch)\n","\n","            train_reg_loss = Lregist.loss(fixed_batch , registered_images)\n","            train_smooth_loss = Lsmooth.loss(fixed_batch , registered_images)\n","            train_loss = -1.0 * train_reg_loss + lamda * train_smooth_loss\n","            train_loss.backward()\n","\n","            optimizer.step()\n","            epoch_loss += train_loss.item()\n","        print(epoch , epoch_loss/len(train_dataset) )"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"D8ADfCQjwLuE","executionInfo":{"status":"ok","timestamp":1620641970551,"user_tz":-270,"elapsed":446873,"user":{"displayName":"elahe badali","photoUrl":"","userId":"17931211121425855261"}}},"source":["model = VxmDense(inshape = (64 , 64),\n","                nb_unet_features=None,\n","                nb_unet_levels=None,\n","                unet_feat_mult=1,\n","                int_steps=1,\n","                int_downsize=1,\n","                bidir=False,\n","                use_probs=False)\n","\n","model = model.to(device)\n"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"0prhbeSJUjcm","executionInfo":{"status":"ok","timestamp":1620641970557,"user_tz":-270,"elapsed":446056,"user":{"displayName":"elahe badali","photoUrl":"","userId":"17931211121425855261"}}},"source":["optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uxwe4HsOUkO9","executionInfo":{"status":"ok","timestamp":1620641970558,"user_tz":-270,"elapsed":445030,"user":{"displayName":"elahe badali","photoUrl":"","userId":"17931211121425855261"}}},"source":["Lregist = NCC(win = [9 , 9])\n","Lsmooth = Grad(penalty ='l2')"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"fUixOIMUwLwM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620641989180,"user_tz":-270,"elapsed":462781,"user":{"displayName":"elahe badali","photoUrl":"","userId":"17931211121425855261"}},"outputId":"c806335a-6476-47bf-9af0-221a3faa0c93"},"source":["train(model , train_loader ,epochs = 50)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"},{"output_type":"stream","text":["0 -0.012529958933591843\n","1 -0.01311676561832428\n","2 -0.013211853951215744\n","3 -0.013267641812562942\n","4 -0.013288931101560592\n","5 -0.013286093845963479\n","6 -0.013350236266851424\n","7 -0.013408442586660385\n","8 -0.013580862358212471\n","9 -0.01394116185605526\n","10 -0.014516581147909165\n","11 -3.3365823984891176\n","12 -0.00403778962790966\n","13 -0.0016822963580489158\n","14 -0.0011476461216807364\n","15 -0.0009828377468511463\n","16 -0.0009351460263133049\n","17 -0.0009197291405871511\n","18 -0.000913036591373384\n","19 -0.0009107905020937323\n","20 -0.0009102887660264969\n","21 -0.00091005505528301\n","22 -0.0009099380252882838\n","23 -0.0009099105698987842\n","24 -0.0009099036827683449\n","25 -0.0009099017968401313\n","26 -0.0009099017549306154\n","27 -0.0009099016943946481\n","28 -0.0009099016105756164\n","29 -0.0009099016105756164\n","30 -0.0009099016431719064\n","31 -0.0009099016059190035\n","32 -0.0009099015640094876\n","33 -0.0009099015686661005\n","34 -0.0009099015779793263\n","35 -0.0009099015733227134\n","36 -0.0009099015826359391\n","37 -0.0009099016012623906\n","38 -0.0009099015407264232\n","39 -0.0009099015919491649\n","40 -0.0009099015686661005\n","41 -0.0009099016059190035\n","42 -0.0009099015640094876\n","43 -0.0009099015593528747\n","44 -0.0009099016012623906\n","45 -0.0009099015826359391\n","46 -0.0009099015919491649\n","47 -0.0009099015966057778\n","48 -0.0009099015779793263\n","49 -0.0009099016012623906\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xAeIMma5ToIO","executionInfo":{"status":"ok","timestamp":1620641989182,"user_tz":-270,"elapsed":461634,"user":{"displayName":"elahe badali","photoUrl":"","userId":"17931211121425855261"}}},"source":["def test(model , test_loader):\n","    with torch.no_grad():\n","        all_loss = 0\n","        for fixed_batch , moving_batch in test_loader:\n","            fixed_batch = fixed_batch.to(device)\n","            moving_batch = moving_batch.to(device)\n","            registered_images ,flow = model(moving_batch , fixed_batch)\n","            test_loss = ncc_loss.loss(fixed_batch , registered_images)\n","            all_loss += test_loss.item()\n","        print(all_loss/len(test_dataset)) \n","        return fixed_batch, moving_batch , registered_images  "],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":358},"id":"aUNJnJjPa_Fx","executionInfo":{"status":"error","timestamp":1620641989930,"user_tz":-270,"elapsed":461423,"user":{"displayName":"elahe badali","photoUrl":"","userId":"17931211121425855261"}},"outputId":"91591379-a7fd-456f-d43b-313d25f488b2"},"source":["fixed_batch, moving_batch , registered_images    = test(model , test_loader)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-146178216ae4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfixed_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoving_batch\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mregistered_images\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-20-b864fc1bcb0f>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, test_loader)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mmoving_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmoving_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mregistered_images\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mflow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoving_batch\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mfixed_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mncc_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixed_batch\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mregistered_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mall_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'ncc_loss' is not defined"]}]},{"cell_type":"code","metadata":{"id":"Ij9sTHRXbBGJ","executionInfo":{"status":"aborted","timestamp":1620641989929,"user_tz":-270,"elapsed":459017,"user":{"displayName":"elahe badali","photoUrl":"","userId":"17931211121425855261"}}},"source":["sample = 5\n","fig , (ax1 , ax2 , ax3) = plt.subplots(1,3,figsize = (10,10))\n","ax1.imshow(moving_batch[sample].squeeze(0).cpu() , cmap = 'gray')\n","ax2.imshow(fixed_batch[sample].squeeze(0).cpu() , cmap = 'gray')\n","ax3.imshow(registered_images[sample].squeeze(0).cpu() , cmap = 'gray')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8DvbR2rGbEaz"},"source":[""],"execution_count":null,"outputs":[]}]}